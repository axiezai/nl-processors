{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('nl-processors': conda)"
    },
    "interpreter": {
      "hash": "eb8be277761d447a0e0b6c238d2c2028680d3d34ee89229561ba5f8717eea730"
    },
    "colab": {
      "name": "classify_gaze.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiezai/nl-processors/blob/main/nl-processors/gaze/colab_classify_gaze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBIC6gIJ4qQY"
      },
      "source": [
        "## Load in GazeCom labeled data for training `[x,y]` coordinates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB6U_1RP4sYh"
      },
      "source": [
        "import requests, zipfile\n",
        "\n",
        "# Download and unzip GazeCom training data in google colab:\n",
        "\n",
        "fname = 'GazeCom.zip'\n",
        "url = 'https://michaeldorr.de/smoothpursuit/GazeCom.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "with open(fname, 'wb') as fd:\n",
        "  fd.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/GazeCom_data')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWpC5-gN4qQg"
      },
      "source": [
        "import os\n",
        "import natsort\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.io import arff\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXC7rF914qQj"
      },
      "source": [
        "Helper functions:    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ELkX_Jb4qQj"
      },
      "source": [
        "def get_files(pattern):\n",
        "    \"\"\"\n",
        "    Extracts file in alphanumerical order that match the provided pattern\n",
        "    \"\"\"\n",
        "    if isinstance(pattern, list):\n",
        "        pattern = os.path.join(*pattern)\n",
        "        \n",
        "    files = natsort.natsorted(glob.glob(pattern))\n",
        "    if not files:\n",
        "        raise FileNotFoundError('Pattern could not detect file(s)')\n",
        "        \n",
        "    return files\n",
        "\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "    if seed is None:\n",
        "        seed = np.random.choice(2 ** 32)\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if seed_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "    print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "def seed_wworker(worker_id):\n",
        "    \"\"\"In case dataloader is used?\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2 ** 32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "    \"\"\"Using GPU or CPU?\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "    if device != 'cuda':\n",
        "      print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "    else:\n",
        "      print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "    return device"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPmgFYR94qQl",
        "outputId": "5b44fb7c-6258-4bc0-a402-eb8290ec3859"
      },
      "source": [
        "DEVICE = set_device()\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n",
            "Random seed 42 has been set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2yrpXqa_tXW"
      },
      "source": [
        "Use `scipy.io.arff` to load example file and convert to `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "C0Ens6Q098n3",
        "outputId": "0587c4c5-bd52-4857-8b9b-b5b9bbe135d8"
      },
      "source": [
        "raw_data = get_files('/content/GazeCom_data/gaze_arff/*/*.arff')\n",
        "print(f'There are {len(raw_data)} raw eye gaze files')\n",
        "\n",
        "labeled_data = get_files('/content/GazeCom_data/ground_truth/*/*.arff')\n",
        "print(f'There are {len(labeled_data)} labeled eye gaze files')\n",
        "\n",
        "# Load one in and examine what's inside ARFF files:\n",
        "raw_arff = arff.loadarff(raw_data[0])\n",
        "raw_df = pd.DataFrame(raw_arff[0])\n",
        "print('Raw file:')\n",
        "raw_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 844 raw eye gaze files\n",
            "There are 844 labeled eye gaze files\n",
            "Raw file:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000.0</td>\n",
              "      <td>590.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13000.0</td>\n",
              "      <td>590.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17000.0</td>\n",
              "      <td>589.8</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      time      x    y  confidence\n",
              "0   1000.0  590.9  5.2         1.0\n",
              "1   5000.0  590.9  5.2         1.0\n",
              "2   9000.0  590.6  5.0         1.0\n",
              "3  13000.0  590.4  5.0         1.0\n",
              "4  17000.0  589.8  5.2         1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "ZnxVh3Il_VYd",
        "outputId": "c6eca24e-0058-4509-89e9-dcabdc3a95a6"
      },
      "source": [
        "labeled_arff = arff.loadarff(labeled_data[0])\n",
        "labeled_df = pd.DataFrame(labeled_arff[0])\n",
        "print('Labeled file:')\n",
        "labeled_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labeled file:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>confidence</th>\n",
              "      <th>handlabeller1</th>\n",
              "      <th>handlabeller2</th>\n",
              "      <th>handlabeller_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000.0</td>\n",
              "      <td>590.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13000.0</td>\n",
              "      <td>590.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17000.0</td>\n",
              "      <td>589.8</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      time      x    y  ...  handlabeller1  handlabeller2  handlabeller_final\n",
              "0   1000.0  590.9  5.2  ...            4.0            4.0                 4.0\n",
              "1   5000.0  590.9  5.2  ...            4.0            4.0                 4.0\n",
              "2   9000.0  590.6  5.0  ...            4.0            4.0                 4.0\n",
              "3  13000.0  590.4  5.0  ...            4.0            4.0                 4.0\n",
              "4  17000.0  589.8  5.2  ...            4.0            4.0                 4.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2hCZwVWAk4L"
      },
      "source": [
        "GazeCom data format: `timestamp in microseconds since start of movie (1e-6)` - `x posititon` - `y position` - `condifence value`. \n",
        "\n",
        "Labels (need to figure out ordering 1-4):\n",
        "\n",
        "\n",
        "*   Fixation\n",
        "*   Saccade\n",
        "*   Smooth pursuit\n",
        "*   noise\n",
        "\n",
        "\n",
        "\n",
        "Acquisition details from paper:\n",
        "\n",
        "\n",
        "*   250Hz sampling rate\n",
        "*   Subjects were 45cm away from screen\n",
        "*   Screen had 40cm width and 30cm height\n",
        "*   Resolution is 1280x960 pixels\n",
        "*   About 26.7 pixels on screen ~ 1 degree of visual angle\n",
        "\n",
        "\n",
        "\n",
        "We hope to train a DL to classify the `<x,y>` positions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxknfS3ZZOx"
      },
      "source": [
        "`extract_windows` function copied over from: [Startsev 2018: Deep eye movement (EM) classifier: a 1D CNN-BLSTM model](https://github.com/MikhailStartsev/deep_em_classifier)\n",
        "\n",
        "`izip_longest` changed  to `zip_longest` in Python3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2GzYVlKZeuy"
      },
      "source": [
        "def zip_equal(*args):\n",
        "    \"\"\"\n",
        "    Iterate the zip-ed combination of @args, making sure they have the same length\n",
        "    :param args: iterables to zip\n",
        "    :return: yields what a usual zip would\n",
        "    \"\"\"\n",
        "    fallback = object()\n",
        "    for combination in itertools.zip_longest(*args, fillvalue=fallback):\n",
        "        if any((c is fallback for c in combination)):\n",
        "            raise ValueError('zip_equals arguments have different length')\n",
        "        yield combination\n",
        "\n",
        "def extract_windows(X, y, window_length,\n",
        "                    padding_features=0,\n",
        "                    downsample=1, temporal_padding=False):\n",
        "    \"\"\"\n",
        "    Extract fixed-sized (@window_length) windows from arbitrary-length sequences (in X and y),\n",
        "    padding them, if necessary (mirror-padding is used).\n",
        "    :param X: input data; list of arrays, each shaped like (NUM_SAMPLES, NUM_FEATURES);\n",
        "              each list item corresponds to one eye tracking recording (one observer & one stimulus clip)\n",
        "    :param y: corresponding labels; list of arrays, each shaped like (NUM_SAMPLES,);\n",
        "              each list element corresponds to sample-level eye movement class labels in the respective sequence;\n",
        "              list elements in X and y are assumed to be matching.\n",
        "    :param window_length: the length of resulting windows; this is the input \"context size\" in the paper, in samples.\n",
        "    :param padding_features: how many extra samples to take in the feature (X) space on each side\n",
        "                             (resulting X will have sequence length longer than resulting Y, by 2 * padding_features,\n",
        "                             while Y will have sample length of @window_length);\n",
        "                             this is necessary due to the use of valid padding in convolution operations in the model;\n",
        "                             if all convolutions are of size 3, and if they all use valid padding, @padding_features\n",
        "                             should be set to the number of convolution layers.\n",
        "    :param downsample: take each @downsample'th window; if equal to @window_length, no overlap between windows;\n",
        "                       by default, all possible windows with the shift of 1 sample between them will be created,\n",
        "                       resulting in NUM_SAMPLES-1 overlap; if overlap of K samples is desired, should set\n",
        "                       @downsample=(NUM_SAMPLES-K)\n",
        "    :param temporal_padding: whether to pad the entire sequences, so that the first window is centered around the\n",
        "                             first sample of the real sequence (i.e. the sequence of recorded eye tracking samples);\n",
        "                             not used\n",
        "    :return: two lists of numpy arrays:\n",
        "                (1) a list of windows corresponding to input data (features),\n",
        "                (2) a list of windows corresponding to labels we will predict.\n",
        "                These can be used as input to network training procedures, for example.\n",
        "    \"\"\"\n",
        "    res_X = []\n",
        "    res_Y = []\n",
        "    # iterate through each file in this subset of videos\n",
        "    for x_item, y_item in zip_equal(X, y):\n",
        "        # pad for all windows\n",
        "        padding_size_x = padding_features\n",
        "        padding_size_y = 0\n",
        "        if temporal_padding:\n",
        "            padding_size_x += window_length / 2\n",
        "            padding_size_y += window_length / 2\n",
        "\n",
        "        padded_x = np.pad(x_item, ((padding_size_x, padding_size_x), (0,0)), 'reflect')\n",
        "        # padded_y = np.pad(y_item, ((padding_size_y, padding_size_y), (0, 0)), 'reflect')\n",
        "        padded_y = np.pad(y_item, (padding_size_y, padding_size_y), 'reflect')\n",
        "        \n",
        "        # Extract all valid windows in @padded_x, with given downsampling and size.\n",
        "        # @res_X will have windows of size @window_length + 2*@padding_features\n",
        "        window_length_x = window_length + 2 * padding_features\n",
        "        res_X += [padded_x[i:i + window_length_x, :] for i in\n",
        "                  range(0, padded_x.shape[0] - window_length_x + 1, downsample)]\n",
        "        # @res_Y will have windows of size @window_length, central to the ones in @res_X\n",
        "        res_Y += [padded_y[i:i + window_length] for i in\n",
        "                  range(0, padded_y.shape[0] - window_length + 1, downsample)]\n",
        "    return res_X, res_Y"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkf6am-tgUf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a9b623-8c87-4103-a10b-3a085694b559"
      },
      "source": [
        "coords = []\n",
        "coords.append(labeled_df[['x', 'y']].values)\n",
        "labels = []\n",
        "bin_labels = labeled_df['handlabeller_final'].values\n",
        "# bin_labels = pd.get_dummies(labeled_df['handlabeller_final']).values\n",
        "labels.append(bin_labels)\n",
        "x, y = extract_windows(coords, labels, 263, padding_features=3)\n",
        "print(len(x), len(y))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4746 4746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YThD5ag7o2x",
        "outputId": "e1ba4ea2-753a-47e0-8eaf-35cdf069ee89"
      },
      "source": [
        "x[0].shape"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(269, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YUZSgZwDdkE"
      },
      "source": [
        "Create DataLoader for dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYjZ8xO6FVto"
      },
      "source": [
        "# Create a custom dataset for GazeCom files:\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GazeComDataset(Dataset):\n",
        "  def __init__(self, data_list, window_size = 257, pad_num = 3, transforms = None):\n",
        "    self.data_list = data_list\n",
        "    self.df = pd.DataFrame(arff.loadarff(data_list[0])[0])\n",
        "    self.win_size = window_size\n",
        "    self.pad_size = pad_num\n",
        "    # self.labels = pd.get_dummies(self.df['handlabeller_final']).values\n",
        "    self.labels = self.df['handlabeller_final'].values\n",
        "    self.transforms = transforms\n",
        "    coords = []\n",
        "    labels = []\n",
        "    coords.append(self.df[['x', 'y']].values)\n",
        "    labels.append(self.labels)\n",
        "    self.window_x, self.window_y = extract_windows(coords, labels, window_length=window_size, padding_features=pad_num)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Return 1 sample and label according to idx:\n",
        "    # print(self.data_list[idx])\n",
        "    data = pd.DataFrame(arff.loadarff(self.data_list[idx])[0])\n",
        "    xy_timeseries = data[['x', 'y']].values\n",
        "    # bin_label = pd.get_dummies(data['handlabeller_final']).values\n",
        "    bin_label = data['handlabeller_final'].values\n",
        "    coords = []\n",
        "    labels = []\n",
        "    coords.append(xy_timeseries)\n",
        "    labels.append(bin_labels)\n",
        "    winx, winy = extract_windows(coords, labels, window_length=self.win_size, padding_features=self.pad_size)\n",
        "    rand_index = torch.randint(low=0, high=min(len(winx), len(winy)), size=(1,))\n",
        "    # print(len(winx), len(winy), rand_index)\n",
        "    return winx[rand_index], winy[rand_index]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrcBtgHxNhDo"
      },
      "source": [
        "Try to iterate through a `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtnxuDY2NgAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10478ca2-c8a1-41e1-e1ca-bae41e311e29"
      },
      "source": [
        "testing_split = 0.2\n",
        "batch_size = 1\n",
        "shuffle = True\n",
        "\n",
        "# create dataset:\n",
        "dataset = GazeComDataset(labeled_data)\n",
        "dataset_size = len(dataset)\n",
        "print(f'Dataset has {dataset_size} .arff files')\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(testing_split * dataset_size))\n",
        "\n",
        "# DataLoader and split:\n",
        "split = int(dataset_size * testing_split)\n",
        "# train_indices, test_indices = indices[split:], indices[:split]\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [dataset_size - split, split])\n",
        "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# Iterate through DataLoader and view an example:\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# labels do not get padded!\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset has 844 .arff files\n",
            "Feature batch shape: torch.Size([1, 263, 2])\n",
            "Labels batch shape: torch.Size([1, 257])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5851-75TL2CK"
      },
      "source": [
        "### Create DL model:\n",
        "\n",
        "Layers: \n",
        "\n",
        "*   Input: 263x2 (windows x features (x,y coordinatets))\n",
        "*   Convolution 1: [263x2, 32@3, 261x32], batch normalization + ReLu\n",
        "*   Convolution 2: [261x32, 16@3, 259x16] .3 dropout, batch normalization + ReLu\n",
        "*   Convolution 3: [259x16, 8@3, 257x8] .3 dropout, batch normalization + ReLu\n",
        "*   TTDLinear: 0.3 dropout, 32,  257x32 output\n",
        "*   BLSTM: tanh activation. 257x32 output\n",
        "*   TTDLLinear: + softmax: 257x4 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVyAwxWL1vZ"
      },
      "source": [
        "# We need a sequence wise module for time distributed fully connected layer\n",
        "class DistributedLinear(nn.Module):\n",
        "  def __init__(self, sequence_length, hidden_size):\n",
        "    super(DistributedLinear, self).__init__()\n",
        "    self.fc_list = nn.ModuleList()\n",
        "    self.seq_length = sequence_length\n",
        "    for j in range(self.seq_length):\n",
        "      fc = nn.Linear(hidden_size, hidden_size)\n",
        "      self.fc_list.append(fc)\n",
        "\n",
        "  def forward(self, x):\n",
        "    lst = []\n",
        "    for j in range(self.seq_length):\n",
        "      lst.append()\n",
        "      out = torch.cat(lst, axais=1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class BLSTM(nn.Module):\n",
        "  def __init__(self, layers, in_sequences, kernel_size = 3, hidden_size=16, drop_p = 0.3):\n",
        "    super(BLSTM, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.n_seqs = in_sequences\n",
        "    self.kernel_size = kernel_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_norm1 = nn.BatchNorm1d(self.n_seqs - kernel_size + 1)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(self.n_seqs - 2*kernel_size + 1)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(self.n_seqs - 3*kernel_size + 1)\n",
        "    self.conv1 = nn.Conv1d(in_channels = self.n_seqs, out_channels = 32, kernel_size = kernel_size)\n",
        "    self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = self.kernel_size)\n",
        "    self.conv3 = nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size = self.kernel_size)\n",
        "    self.fc1   = DistributedLinear(257, 32)\n",
        "    self.fc2   = DistributedLinear(257, 4)\n",
        "    self.lstm  = nn.LSTM(input_size = 257, hidden_size= 16, bidirectional = True)\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    \"\"\"\n",
        "    Input is a window length x batch size tensor\n",
        "    \"\"\"\n",
        "    # Randomly initialize weights for LSTM, need to confirm shape:\n",
        "    hidden = torch.randn(1, 257, self.hidden_size).to('cpu')\n",
        "    x = self.conv1(input)\n",
        "    x = self.batch_norm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.batch_norm3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    hidden_states, cell_states = self.lstm(x, hidden) # How does it work for 1 layer x 257 sequences x 32 features?\n",
        "    x = F.tanh(cell_states)\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x)\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySX_MC8p01-O"
      },
      "source": [
        "x, y = next(iter(dataset))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnbOtwp21jjY"
      },
      "source": [
        "torch.tensor(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP5nAmDu1kwq",
        "outputId": "c8fd5a81-cf39-4904-c0d4-92441627c0a5"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(263, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "hSLJ_dF4fUv1",
        "outputId": "23a1f3f9-ef94-494f-d598-6537265aeaad"
      },
      "source": [
        "testLSTM = BLSTM(1, 263)\n",
        "testLSTM(torch.tensor(x))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-47bad201a779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m263\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtestLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-143-0ef7c42d86e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Randomly initialize weights for LSTM, need to confirm shape:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m257\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    294\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 295\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [32, 263, 3], but got 2-dimensional input of size [263, 2] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_nv2R_y4bo0"
      },
      "source": [
        "m = nn.BatchNorm1d(100, affine=False)\n",
        "input = torch.randn(20, 100)\n",
        "output = m(input)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaBI994khmWI"
      },
      "source": [
        "Create training and testing functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysog8a5Ng3u4"
      },
      "source": [
        "def train(model, device, train_dataloader, epochs, learning_rate):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # define hyperparameters\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam()\n",
        "\n",
        "  # prep lists\n",
        "  train_loss, train_acc = [], []\n",
        "\n",
        "  # training loop:\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    steps = 0\n",
        "\n",
        "    # iterate over training data loader:\n",
        "    for idx, data in enumerate(train_dataloader):\n",
        "      eye_coords, labels = data\n",
        "      \n",
        "\n",
        "def test():\n",
        "  \"\"\"\n",
        "  \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}