{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('nl-processors': conda)"
    },
    "interpreter": {
      "hash": "eb8be277761d447a0e0b6c238d2c2028680d3d34ee89229561ba5f8717eea730"
    },
    "colab": {
      "name": "classify_gaze.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "374387c30ade4ba88a7350b9ddf0beba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6bc6bbd954e2459f8ad90b38c3e2ba8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_edf15814e23e4b4688cc6f9960d811e9",
              "IPY_MODEL_10755e0660244e69a9924cfa7d00e564",
              "IPY_MODEL_eb3e7780aa90407d8606a64b618b13de"
            ]
          }
        },
        "6bc6bbd954e2459f8ad90b38c3e2ba8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "edf15814e23e4b4688cc6f9960d811e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e88ec43ddd7b437ebbd9c5c652d21961",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epochs:   5%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9acbccbb54647869182a31fd6bf75dd"
          }
        },
        "10755e0660244e69a9924cfa7d00e564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4cfc5036a2904799954ef991026015b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c81b2d6901b14a348ed81a84bcd1cc43"
          }
        },
        "eb3e7780aa90407d8606a64b618b13de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9531d117ce7041d0af391c5d99717258",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/20 [01:56&lt;33:56, 107.16s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74a611b567a2445b8c6039810998f0b7"
          }
        },
        "e88ec43ddd7b437ebbd9c5c652d21961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9acbccbb54647869182a31fd6bf75dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cfc5036a2904799954ef991026015b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c81b2d6901b14a348ed81a84bcd1cc43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9531d117ce7041d0af391c5d99717258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74a611b567a2445b8c6039810998f0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiezai/nl-processors/blob/main/nl-processors/gaze/colab_classify_gaze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBIC6gIJ4qQY"
      },
      "source": [
        "## Load in GazeCom labeled data for training `[x,y]` coordinates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB6U_1RP4sYh"
      },
      "source": [
        "import requests, zipfile\n",
        "\n",
        "# Download and unzip GazeCom training data in google colab:\n",
        "\n",
        "fname = 'GazeCom.zip'\n",
        "url = 'https://michaeldorr.de/smoothpursuit/GazeCom.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "with open(fname, 'wb') as fd:\n",
        "  fd.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/GazeCom_data')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWpC5-gN4qQg"
      },
      "source": [
        "import os\n",
        "import natsort\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.io import arff\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXC7rF914qQj"
      },
      "source": [
        "Helper functions:    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ELkX_Jb4qQj"
      },
      "source": [
        "def get_files(pattern):\n",
        "    \"\"\"\n",
        "    Extracts file in alphanumerical order that match the provided pattern\n",
        "    \"\"\"\n",
        "    if isinstance(pattern, list):\n",
        "        pattern = os.path.join(*pattern)\n",
        "        \n",
        "    files = natsort.natsorted(glob.glob(pattern))\n",
        "    if not files:\n",
        "        raise FileNotFoundError('Pattern could not detect file(s)')\n",
        "        \n",
        "    return files\n",
        "\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "    if seed is None:\n",
        "        seed = np.random.choice(2 ** 32)\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if seed_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "    print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "def seed_wworker(worker_id):\n",
        "    \"\"\"In case dataloader is used?\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2 ** 32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "    \"\"\"Using GPU or CPU?\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "    if device != 'cuda':\n",
        "      print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "    else:\n",
        "      print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "    return device"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPmgFYR94qQl",
        "outputId": "f13f9812-f3b3-4be4-c4d0-3592e4704621"
      },
      "source": [
        "DEVICE = set_device()\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is enabled in this notebook.\n",
            "Random seed 42 has been set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2yrpXqa_tXW"
      },
      "source": [
        "Use `scipy.io.arff` to load example file and convert to `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "C0Ens6Q098n3",
        "outputId": "117ae0cf-f73f-439c-e365-22acefadb26e"
      },
      "source": [
        "raw_data = get_files('/content/GazeCom_data/gaze_arff/*/*.arff')\n",
        "print(f'There are {len(raw_data)} raw eye gaze files')\n",
        "\n",
        "labeled_data = get_files('/content/GazeCom_data/ground_truth/*/*.arff')\n",
        "print(f'There are {len(labeled_data)} labeled eye gaze files')\n",
        "\n",
        "# Load one in and examine what's inside ARFF files:\n",
        "raw_arff = arff.loadarff(raw_data[0])\n",
        "raw_df = pd.DataFrame(raw_arff[0])\n",
        "print('Raw file:')\n",
        "raw_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 844 raw eye gaze files\n",
            "There are 844 labeled eye gaze files\n",
            "Raw file:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000.0</td>\n",
              "      <td>590.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13000.0</td>\n",
              "      <td>590.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17000.0</td>\n",
              "      <td>589.8</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      time      x    y  confidence\n",
              "0   1000.0  590.9  5.2         1.0\n",
              "1   5000.0  590.9  5.2         1.0\n",
              "2   9000.0  590.6  5.0         1.0\n",
              "3  13000.0  590.4  5.0         1.0\n",
              "4  17000.0  589.8  5.2         1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "ZnxVh3Il_VYd",
        "outputId": "f08a11d9-3011-40b9-af23-ca33fc169914"
      },
      "source": [
        "labeled_arff = arff.loadarff(labeled_data[0])\n",
        "labeled_df = pd.DataFrame(labeled_arff[0])\n",
        "print('Labeled file:')\n",
        "labeled_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labeled file:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>confidence</th>\n",
              "      <th>handlabeller1</th>\n",
              "      <th>handlabeller2</th>\n",
              "      <th>handlabeller_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>590.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9000.0</td>\n",
              "      <td>590.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13000.0</td>\n",
              "      <td>590.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17000.0</td>\n",
              "      <td>589.8</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      time      x    y  ...  handlabeller1  handlabeller2  handlabeller_final\n",
              "0   1000.0  590.9  5.2  ...            4.0            4.0                 4.0\n",
              "1   5000.0  590.9  5.2  ...            4.0            4.0                 4.0\n",
              "2   9000.0  590.6  5.0  ...            4.0            4.0                 4.0\n",
              "3  13000.0  590.4  5.0  ...            4.0            4.0                 4.0\n",
              "4  17000.0  589.8  5.2  ...            4.0            4.0                 4.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2hCZwVWAk4L"
      },
      "source": [
        "GazeCom data format: `timestamp in microseconds since start of movie (1e-6)` - `x posititon` - `y position` - `condifence value`. \n",
        "\n",
        "Labels (need to figure out ordering 1-4):\n",
        "\n",
        "\n",
        "*   Fixation\n",
        "*   Saccade\n",
        "*   Smooth pursuit\n",
        "*   noise\n",
        "\n",
        "\n",
        "\n",
        "Acquisition details from paper:\n",
        "\n",
        "\n",
        "*   250Hz sampling rate\n",
        "*   Subjects were 45cm away from screen\n",
        "*   Screen had 40cm width and 30cm height\n",
        "*   Resolution is 1280x960 pixels\n",
        "*   About 26.7 pixels on screen ~ 1 degree of visual angle\n",
        "\n",
        "\n",
        "\n",
        "We hope to train a DL to classify the `<x,y>` positions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxknfS3ZZOx"
      },
      "source": [
        "`extract_windows` function copied over from: [Startsev 2018: Deep eye movement (EM) classifier: a 1D CNN-BLSTM model](https://github.com/MikhailStartsev/deep_em_classifier)\n",
        "\n",
        "`izip_longest` changed  to `zip_longest` in Python3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2GzYVlKZeuy"
      },
      "source": [
        "def zip_equal(*args):\n",
        "    \"\"\"\n",
        "    Iterate the zip-ed combination of @args, making sure they have the same length\n",
        "    :param args: iterables to zip\n",
        "    :return: yields what a usual zip would\n",
        "    \"\"\"\n",
        "    fallback = object()\n",
        "    for combination in itertools.zip_longest(*args, fillvalue=fallback):\n",
        "        if any((c is fallback for c in combination)):\n",
        "            raise ValueError('zip_equals arguments have different length')\n",
        "        yield combination\n",
        "\n",
        "def extract_windows(X, y, window_length,\n",
        "                    padding_features=0,\n",
        "                    downsample=1, temporal_padding=False):\n",
        "    \"\"\"\n",
        "    Extract fixed-sized (@window_length) windows from arbitrary-length sequences (in X and y),\n",
        "    padding them, if necessary (mirror-padding is used).\n",
        "    :param X: input data; list of arrays, each shaped like (NUM_SAMPLES, NUM_FEATURES);\n",
        "              each list item corresponds to one eye tracking recording (one observer & one stimulus clip)\n",
        "    :param y: corresponding labels; list of arrays, each shaped like (NUM_SAMPLES,);\n",
        "              each list element corresponds to sample-level eye movement class labels in the respective sequence;\n",
        "              list elements in X and y are assumed to be matching.\n",
        "    :param window_length: the length of resulting windows; this is the input \"context size\" in the paper, in samples.\n",
        "    :param padding_features: how many extra samples to take in the feature (X) space on each side\n",
        "                             (resulting X will have sequence length longer than resulting Y, by 2 * padding_features,\n",
        "                             while Y will have sample length of @window_length);\n",
        "                             this is necessary due to the use of valid padding in convolution operations in the model;\n",
        "                             if all convolutions are of size 3, and if they all use valid padding, @padding_features\n",
        "                             should be set to the number of convolution layers.\n",
        "    :param downsample: take each @downsample'th window; if equal to @window_length, no overlap between windows;\n",
        "                       by default, all possible windows with the shift of 1 sample between them will be created,\n",
        "                       resulting in NUM_SAMPLES-1 overlap; if overlap of K samples is desired, should set\n",
        "                       @downsample=(NUM_SAMPLES-K)\n",
        "    :param temporal_padding: whether to pad the entire sequences, so that the first window is centered around the\n",
        "                             first sample of the real sequence (i.e. the sequence of recorded eye tracking samples);\n",
        "                             not used\n",
        "    :return: two lists of numpy arrays:\n",
        "                (1) a list of windows corresponding to input data (features),\n",
        "                (2) a list of windows corresponding to labels we will predict.\n",
        "                These can be used as input to network training procedures, for example.\n",
        "    \"\"\"\n",
        "    res_X = []\n",
        "    res_Y = []\n",
        "    # iterate through each file in this subset of videos\n",
        "    for x_item, y_item in zip_equal(X, y):\n",
        "        # pad for all windows\n",
        "        padding_size_x = padding_features\n",
        "        padding_size_y = 0\n",
        "        if temporal_padding:\n",
        "            padding_size_x += window_length / 2\n",
        "            padding_size_y += window_length / 2\n",
        "\n",
        "        padded_x = np.pad(x_item, ((padding_size_x, padding_size_x), (0,0)), 'reflect')\n",
        "        # padded_y = np.pad(y_item, ((padding_size_y, padding_size_y), (0, 0)), 'reflect')\n",
        "        padded_y = np.pad(y_item, (padding_size_y, padding_size_y), 'reflect')\n",
        "        \n",
        "        # Extract all valid windows in @padded_x, with given downsampling and size.\n",
        "        # @res_X will have windows of size @window_length + 2*@padding_features\n",
        "        window_length_x = window_length + 2 * padding_features\n",
        "        res_X += [padded_x[i:i + window_length_x, :] for i in\n",
        "                  range(0, padded_x.shape[0] - window_length_x + 1, downsample)]\n",
        "        # @res_Y will have windows of size @window_length, central to the ones in @res_X\n",
        "        res_Y += [padded_y[i:i + window_length] for i in\n",
        "                  range(0, padded_y.shape[0] - window_length + 1, downsample)]\n",
        "    return res_X, res_Y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bkf6am-tgUf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7831f1a2-8b61-499c-d54e-90ce1bfe020d"
      },
      "source": [
        "coords = []\n",
        "coords.append(labeled_df[['x', 'y']].values)\n",
        "labels = []\n",
        "bin_labels = labeled_df['handlabeller_final'].values\n",
        "# bin_labels = pd.get_dummies(labeled_df['handlabeller_final']).values\n",
        "labels.append(bin_labels)\n",
        "x, y = extract_windows(coords, labels, 263, padding_features=3)\n",
        "print(len(x), len(y))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4746 4746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YUZSgZwDdkE"
      },
      "source": [
        "Create DataLoader for dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYjZ8xO6FVto"
      },
      "source": [
        "# Create a custom dataset for GazeCom files:\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GazeComDataset(Dataset):\n",
        "  def __init__(self, data_list, window_size = 129, pad_num = 3, transforms = None):\n",
        "    self.data_list = data_list\n",
        "    self.df = pd.DataFrame(arff.loadarff(data_list[0])[0])\n",
        "    self.win_size = window_size\n",
        "    self.pad_size = pad_num\n",
        "    # self.labels = pd.get_dummies(self.df['handlabeller_final']).values\n",
        "    self.labels = self.df['handlabeller_final'].values\n",
        "    self.transforms = transforms\n",
        "    coords = []\n",
        "    labels = []\n",
        "    coords.append(self.df[['x', 'y']].values)\n",
        "    labels.append(self.labels)\n",
        "    self.window_x, self.window_y = extract_windows(coords, labels, window_length=window_size, padding_features=pad_num)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Return 1 sample and label according to idx:\n",
        "    # print(self.data_list[idx])\n",
        "    data = pd.DataFrame(arff.loadarff(self.data_list[idx])[0])\n",
        "    xy_timeseries = data[['x', 'y']].values\n",
        "    # bin_label = pd.get_dummies(data['handlabeller_final']).values\n",
        "    bin_label = data['handlabeller_final'].values\n",
        "    coords = []\n",
        "    labels = []\n",
        "    coords.append(xy_timeseries)\n",
        "    labels.append(bin_labels)\n",
        "    winx, winy = extract_windows(coords, labels, window_length=self.win_size, padding_features=self.pad_size)\n",
        "    rand_index = torch.randint(low=0, high=min(len(winx), len(winy)), size=(1,))\n",
        "    # print(len(winx), len(winy), rand_index)\n",
        "    return winx[rand_index], winy[rand_index]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrcBtgHxNhDo"
      },
      "source": [
        "Try to iterate through a `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtnxuDY2NgAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d53ae8-aee8-4088-9108-e5c81ff3944a"
      },
      "source": [
        "testing_split = 0.2\n",
        "batch_size = 1\n",
        "shuffle = True\n",
        "\n",
        "# create dataset:\n",
        "dataset = GazeComDataset(labeled_data)\n",
        "dataset_size = len(dataset)\n",
        "print(f'Dataset has {dataset_size} .arff files')\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(testing_split * dataset_size))\n",
        "\n",
        "# DataLoader and split:\n",
        "split = int(dataset_size * testing_split)\n",
        "# train_indices, test_indices = indices[split:], indices[:split]\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [dataset_size - split, split])\n",
        "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# Iterate through DataLoader and view an example:\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# labels do not get padded!\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset has 844 .arff files\n",
            "Feature batch shape: torch.Size([1, 135, 2])\n",
            "Labels batch shape: torch.Size([1, 129])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5851-75TL2CK"
      },
      "source": [
        "### Create DL model:\n",
        "\n",
        "Layers: \n",
        "\n",
        "*   Input: 263x2 (windows x features (x,y coordinatets))\n",
        "*   Convolution 1: [263x2, 32@3, 261x32], batch normalization + ReLu\n",
        "*   Convolution 2: [261x32, 16@3, 259x16] .3 dropout, batch normalization + ReLu\n",
        "*   Convolution 3: [259x16, 8@3, 257x8] .3 dropout, batch normalization + ReLu\n",
        "*   TTDLinear: 0.3 dropout, 32,  257x32 output\n",
        "*   BLSTM: tanh activation. 257x32 output\n",
        "*   TTDLLinear: + softmax: 257x4 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVyAwxWL1vZ"
      },
      "source": [
        "# We need a sequence wise module for time distributed fully connected layer\n",
        "class DistributedLinear(nn.Module):\n",
        "  def __init__(self, sequence_length, hidden_size):\n",
        "    super(DistributedLinear, self).__init__()\n",
        "    self.fc_list = nn.ModuleList()\n",
        "    self.seq_length = sequence_length\n",
        "    for j in range(self.seq_length):\n",
        "      fc = nn.Linear(self.seq_length, hidden_size)\n",
        "      self.fc_list.append(fc)\n",
        "\n",
        "  def forward(self, x):\n",
        "    lst = []\n",
        "    for j in range(self.seq_length):\n",
        "      # print(x[:, j:j+1,:].shape)\n",
        "      lst.append(self.fc_list[j](x[:, j:j+1,:]))\n",
        "      # lst.append(self.fc_list[j](x[:,:,j:j+1]))\n",
        "    out = torch.cat(lst, dim=1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class BLSTM(nn.Module):\n",
        "  def __init__(self, layers, in_sequences, device, kernel_size = 3, hidden_size=16, drop_p = 0.3):\n",
        "    super(BLSTM, self).__init__()\n",
        "    self.n_layers = layers\n",
        "    self.n_seqs = in_sequences\n",
        "    self.device = device\n",
        "    self.kernel_size = kernel_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batch_norm1 = nn.BatchNorm1d(32)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(16)\n",
        "    self.batch_norm3 = nn.BatchNorm1d(8)\n",
        "    self.conv1 = nn.Conv1d(in_channels = 2, out_channels = 32, kernel_size = self.kernel_size)\n",
        "    self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = self.kernel_size)\n",
        "    self.conv3 = nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size = self.kernel_size)\n",
        "    self.fc1   = DistributedLinear(8, 32)\n",
        "    self.fc2   = DistributedLinear(32, 4)\n",
        "    self.lstm  = nn.LSTM(input_size = 32, hidden_size= hidden_size, num_layers=16, batch_first=False, bidirectional = True)\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    \"\"\"\n",
        "    Input is a window length x batch size tensor\n",
        "    \"\"\"\n",
        "    x = input.view(1, 2, self.n_seqs) # reshape for cov?\n",
        "    # Randomly initialize weights for LSTM, need to confirm shape:\n",
        "    hidden = (torch.randn(2 * 16, 1, self.hidden_size).double().to(self.device),\n",
        "              torch.randn(2 * 16, 1, self.hidden_size).double().to(self.device))\n",
        "    x = self.conv1(x)\n",
        "    x = self.batch_norm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.batch_norm3(x)\n",
        "    x = F.relu(x)\n",
        "    x = x.permute(2, 0, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(x)\n",
        "    output, (hn, cn) = self.lstm(x, hidden)\n",
        "    x = torch.tanh(output)\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x, dim=0).permute(1,2,0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSLJ_dF4fUv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5d52c4-1242-4345-8a7f-332b93a5839b"
      },
      "source": [
        "gazecom_bLSTM = BLSTM(1, 135, device = DEVICE)\n",
        "gazecom_bLSTM = gazecom_bLSTM.double().to(DEVICE)\n",
        "logits = gazecom_bLSTM(train_features.to(DEVICE)) # Permuted for CrossEntropyLoss: minibatches x classes x seq\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "train_labels = train_labels.long().to(DEVICE)\n",
        "output = loss(logits, train_labels-1)\n",
        "print(output)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.3863, device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<NllLoss2DBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaBI994khmWI"
      },
      "source": [
        "Create training and testing functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysog8a5Ng3u4"
      },
      "source": [
        "def train(model, device, train_dataloader, epochs, learning_rate):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # define hyperparameters\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "  # prep lists\n",
        "  train_loss, train_acc = [], []\n",
        "\n",
        "  # training loop:\n",
        "  for epoch in tqdm(range(epochs), desc= 'Epochs'):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    steps = 0\n",
        "\n",
        "    # iterate over training data loader:\n",
        "    for idx, data in enumerate(train_dataloader):\n",
        "      eye_coords, labels = data\n",
        "      eye_coords, labels = eye_coords.to(device), labels.long().to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      out_prob = model(eye_coords)\n",
        "      # Labels needs to be converted to dtype `long` and -1 for C-1 classes\n",
        "      labels = labels -1\n",
        "      loss = criterion(out_prob, labels)\n",
        "      loss.backward()\n",
        "      steps += 1\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # get accuracy:\n",
        "      _, predicted = torch.max(out_prob.squeeze(), 0)\n",
        "      total += labels.size(1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    train_loss.append(running_loss/len(train_dataloader))\n",
        "    train_acc.append(correct/total)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test(model, device, test_dataloader):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for idx, data in enumerate(test_dataloader):\n",
        "      eye_coords, labels = data\n",
        "      eye_coords, labels = eye_coords.to(device), labels.to(device)\n",
        "      out_prob = model(eye_coords)\n",
        "      \n",
        "      _, predicted = torch.max(out_prob.squeeze(), dim = 0)\n",
        "      total += labels.size(1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * (correct/total)\n",
        "    return acc"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmsczhIYpEFk"
      },
      "source": [
        "Train on GazeCom:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464,
          "referenced_widgets": [
            "374387c30ade4ba88a7350b9ddf0beba",
            "6bc6bbd954e2459f8ad90b38c3e2ba8b",
            "edf15814e23e4b4688cc6f9960d811e9",
            "10755e0660244e69a9924cfa7d00e564",
            "eb3e7780aa90407d8606a64b618b13de",
            "e88ec43ddd7b437ebbd9c5c652d21961",
            "b9acbccbb54647869182a31fd6bf75dd",
            "4cfc5036a2904799954ef991026015b3",
            "c81b2d6901b14a348ed81a84bcd1cc43",
            "9531d117ce7041d0af391c5d99717258",
            "74a611b567a2445b8c6039810998f0b7"
          ]
        },
        "id": "G7l1isRYoGez",
        "outputId": "01ef496a-f07a-4235-ef33-87f6b4ec356b"
      },
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "print(f'Training on {DEVICE}')\n",
        "\n",
        "gazecom_bLSTM.to(DEVICE)\n",
        "train_loss, train_acc = train(gazecom_bLSTM, DEVICE, train_loader, epochs, learning_rate)\n",
        "test_accuracy = test(gazecom_bLSTM, DEVICE, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy} after training for {epochs} epochs.')\n",
        "\n",
        "# plot accuracy:\n",
        "plt.figure()\n",
        "plt.plot(np.arange(epochs), train_acc, label = 'train accuracy', linestyle = '--')\n",
        "plt.plot(np.arange(epochs), test_accuracy, label = 'test accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "374387c30ade4ba88a7350b9ddf0beba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-60a4ab9c0d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgazecom_bLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgazecom_bLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgazecom_bLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test accuracy: {test_accuracy} after training for {epochs} epochs.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-cdc77a514148>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_dataloader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOBltNZAvop0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}